pub mod grammars;
// pub mod textmate;
pub mod themes;
// mod tokenizer;

// #[cfg(test)]
// mod snapshot_tests {
//     use crate::grammars::{RawGrammar, ScopeId};
//     use crate::textmate::tokenizer::Tokenizer;
//     use crate::theme::{RawTheme, StyleCache};
//     use std::fs;
//
//     /// Helper function to convert ScopeId back to scope names for debugging
//     /// This is inefficient but useful for debugging
//     fn scope_stack_to_names(scope_stack: &[ScopeId]) -> Vec<String> {
//         scope_stack
//             .iter()
//             .map(|scope_id| {
//                 // Search through the PHF map to find the name for this ID
//                 // This is O(n) but only used for debugging
//                 use crate::textmate::grammar::SCOPE_MAP;
//                 for (name, &id) in SCOPE_MAP.entries() {
//                     if ScopeId(id) == *scope_id {
//                         return name.to_string();
//                     }
//                 }
//                 format!("Unknown({})", scope_id.0)
//             })
//             .collect()
//     }
//
//     fn load_grammar(
//         grammar_name: &str,
//     ) -> Result<crate::textmate::grammar::CompiledGrammar, Box<dyn std::error::Error>> {
//         let grammar_path = format!(
//             "grammars-themes/packages/tm-grammars/grammars/{}.json",
//             grammar_name
//         );
//         let raw_grammar = RawGrammar::load_from_json_file(&grammar_path)?;
//         let compiled_grammar = raw_grammar.compile()?;
//         Ok(compiled_grammar)
//     }
//
//     fn load_vitesse_black_theme() -> Result<crate::theme::CompiledTheme, Box<dyn std::error::Error>>
//     {
//         let theme = RawTheme::load_builtin("vitesse-black")?;
//         let compiled_theme = theme.compile()?;
//         Ok(compiled_theme)
//     }
//
//     fn tokenize_and_format(
//         content: &str,
//         grammar_name: &str,
//     ) -> Result<String, Box<dyn std::error::Error>> {
//         let grammar = load_grammar(grammar_name)?;
//         let theme = load_vitesse_black_theme()?;
//         let mut tokenizer = Tokenizer::new(grammar);
//         let mut style_cache = StyleCache::new();
//
//         let mut result_lines = Vec::new();
//
//         for (line_idx, line) in content.lines().enumerate() {
//             let tokens = tokenizer.tokenize_line(line)?;
//
//             // Debug: Show token details for first few lines
//             if line_idx < 2 {
//                 println!(
//                     "    [DEBUG] Line {}: '{}' -> {} tokens",
//                     line_idx,
//                     line,
//                     tokens.len()
//                 );
//                 for (i, token) in tokens.iter().enumerate().take(3) {
//                     let token_text = line.get(token.start..token.end).unwrap_or("<invalid>");
//                     let scope_names = scope_stack_to_names(&token.scope_stack);
//                     println!(
//                         "      Token {}: {}..{} = '{}' | Scopes: {:?}",
//                         i, token.start, token.end, token_text, scope_names
//                     );
//                 }
//             }
//
//             let batched_tokens = Tokenizer::batch_tokens(&tokens, &theme, &mut style_cache);
//
//             for batch in batched_tokens {
//                 if let Some(style) = style_cache.get_style(batch.style_id) {
//                     // Format color as hex string for display
//                     let color = format!(
//                         "#{:02x}{:02x}{:02x}",
//                         style.foreground.r, style.foreground.g, style.foreground.b
//                     );
//                     let text = line
//                         .get(batch.start as usize..batch.end as usize)
//                         .unwrap_or("");
//                     let formatted_line = format!("{:<15}{}", color, text);
//                     result_lines.push(formatted_line);
//                 }
//             }
//         }
//
//         Ok(result_lines.join("\n"))
//     }
//
//     #[test]
//     fn test_javascript_comment_scopes() {
//         println!("=== TESTING JAVASCRIPT COMMENT SCOPE GENERATION ===");
//
//         let test_text = "// this is a comment";
//
//         match load_grammar("javascript") {
//             Ok(grammar) => {
//                 println!("JavaScript Grammar loaded successfully");
//
//                 // Check if we can find comment patterns in the compiled grammar
//                 let mut comment_patterns_found = 0;
//                 let mut arithmetic_patterns_found = 0;
//                 println!("\n=== SEARCHING FOR COMMENT PATTERNS IN COMPILED GRAMMAR ===");
//                 for (i, pattern) in grammar.patterns.iter().enumerate().take(5) {
//                     match pattern {
//                         crate::textmate::grammar::CompiledPattern::Include(inc) => {
//                             println!(
//                                 "Root pattern {}: Include with {} sub-patterns",
//                                 i,
//                                 inc.patterns.len()
//                             );
//                             // Search for comment patterns in includes
//                             search_patterns_for_comments(
//                                 &inc.patterns,
//                                 &mut comment_patterns_found,
//                                 &mut arithmetic_patterns_found,
//                                 0,
//                             );
//                         }
//                         crate::textmate::grammar::CompiledPattern::Match(m) => {
//                             let regex_str = m.regex.pattern();
//                             if regex_str.contains("//") || regex_str.contains("comment") {
//                                 comment_patterns_found += 1;
//                                 println!("  Found comment-related Match: {}", regex_str);
//                             }
//                             if regex_str.contains("[-%*+/]") || regex_str.contains("/") {
//                                 arithmetic_patterns_found += 1;
//                                 println!("  Found arithmetic Match: {}", regex_str);
//                             }
//                         }
//                         _ => {}
//                     }
//                 }
//
//                 println!("Total comment patterns found: {}", comment_patterns_found);
//                 println!(
//                     "Total arithmetic patterns found: {}",
//                     arithmetic_patterns_found
//                 );
//
//                 let mut tokenizer = Tokenizer::new(grammar);
//                 match tokenizer.tokenize_line(test_text) {
//                     Ok(tokens) => {
//                         println!("\nTokenized '{}' into {} tokens:", test_text, tokens.len());
//                         for (i, token) in tokens.iter().enumerate() {
//                             let token_text =
//                                 test_text.get(token.start..token.end).unwrap_or("<invalid>");
//                             let scope_names = scope_stack_to_names(&token.scope_stack);
//                             println!(
//                                 "  Token {}: '{}' | Scopes: {:?}",
//                                 i, token_text, scope_names
//                             );
//                         }
//                     }
//                     Err(e) => {
//                         println!("Tokenization failed: {}", e);
//                     }
//                 }
//             }
//             Err(e) => {
//                 println!("Failed to load JavaScript grammar: {}", e);
//             }
//         }
//     }
//
//     // Helper function to recursively search for comment patterns
//     fn search_patterns_for_comments(
//         patterns: &[crate::textmate::grammar::CompiledPattern],
//         comment_count: &mut i32,
//         arithmetic_count: &mut i32,
//         depth: usize,
//     ) {
//         if depth > 8 {
//             return;
//         } // Prevent infinite recursion, but go deeper
//
//         for (i, pattern) in patterns.iter().enumerate().take(50) {
//             // Look at more patterns
//             match pattern {
//                 crate::textmate::grammar::CompiledPattern::Include(inc) => {
//                     println!(
//                         "    {}Include at depth {} index {}: {} sub-patterns",
//                         "  ".repeat(depth),
//                         depth,
//                         i,
//                         inc.patterns.len()
//                     );
//                     search_patterns_for_comments(
//                         &inc.patterns,
//                         comment_count,
//                         arithmetic_count,
//                         depth + 1,
//                     );
//                 }
//                 crate::textmate::grammar::CompiledPattern::Match(m) => {
//                     let regex_str = m.regex.pattern();
//
//                     // Show ALL patterns at key depths (where we found arithmetic patterns)
//                     if depth == 5 {
//                         println!(
//                             "    {}üìç Pattern at critical depth {} index {}: {}",
//                             "  ".repeat(depth),
//                             depth,
//                             i,
//                             regex_str
//                         );
//                     }
//
//                     if regex_str.contains("//")
//                         || regex_str.contains("comment")
//                         || regex_str.contains("/\\*")
//                     {
//                         *comment_count += 1;
//                         println!(
//                             "    {}‚úì Found comment Match (depth {} index {}): {}",
//                             "  ".repeat(depth),
//                             depth,
//                             i,
//                             regex_str
//                         );
//                     }
//                     if regex_str.contains("[-%*+/]")
//                         || (regex_str.contains("/") && regex_str.len() < 50)
//                     {
//                         *arithmetic_count += 1;
//                         println!(
//                             "    {}‚úì Found arithmetic Match (depth {} index {}): {}",
//                             "  ".repeat(depth),
//                             depth,
//                             i,
//                             regex_str
//                         );
//                     }
//                     // Also show identifier patterns that might be matching wrongly
//                     if regex_str.contains("[$_[:alpha:]]") && depth <= 6 {
//                         println!(
//                             "    {}‚Ä¢ Identifier pattern (depth {} index {}): {}",
//                             "  ".repeat(depth),
//                             depth,
//                             i,
//                             regex_str
//                         );
//                     }
//                 }
//                 crate::textmate::grammar::CompiledPattern::BeginEnd(be) => {
//                     let begin_regex = be.begin_regex.pattern();
//
//                     // Show ALL BeginEnd patterns at key depths
//                     if depth == 5 {
//                         println!(
//                             "    {}üìç BeginEnd at critical depth {} index {}: {}",
//                             "  ".repeat(depth),
//                             depth,
//                             i,
//                             begin_regex
//                         );
//                     }
//
//                     if begin_regex.contains("//")
//                         || begin_regex.contains("comment")
//                         || begin_regex.contains("/\\*")
//                     {
//                         *comment_count += 1;
//                         println!(
//                             "    {}‚úì Found comment BeginEnd (depth {} index {}): {}",
//                             "  ".repeat(depth),
//                             depth,
//                             i,
//                             begin_regex
//                         );
//                     }
//                     if begin_regex.contains("/") && begin_regex.len() < 100 {
//                         println!(
//                             "    {}‚Ä¢ BeginEnd with / (depth {} index {}): {}",
//                             "  ".repeat(depth),
//                             depth,
//                             i,
//                             begin_regex
//                         );
//                     }
//                 }
//                 _ => {}
//             }
//         }
//     }
//
//     #[test]
//     fn test_regex_positions_debug() {
//         println!("=== DEBUGGING REGEX POSITION CALCULATION ===");
//
//         let test_text = "CND ‚Üê {";
//         println!(
//             "Original text: '{}' ({} bytes, {} chars)",
//             test_text,
//             test_text.len(),
//             test_text.chars().count()
//         );
//
//         // Show byte positions of each character
//         for (char_idx, (byte_idx, ch)) in test_text.char_indices().enumerate() {
//             println!("  Char {}: '{}' at byte {}", char_idx, ch, byte_idx);
//         }
//
//         // Test what happens when we slice at different positions
//         for start_pos in 0..test_text.len() {
//             if let Some(slice) = test_text.get(start_pos..) {
//                 println!(
//                     "Slice from byte {}: '{}' ({} bytes)",
//                     start_pos,
//                     slice,
//                     slice.len()
//                 );
//
//                 // Try a simple regex match on the slice
//                 if let Ok(regex) = onig::Regex::new(r"\S+") {
//                     if let Some(captures) = regex.captures(slice) {
//                         if let Some(match_pos) = captures.pos(0) {
//                             println!("  Regex match: {}..{} in slice", match_pos.0, match_pos.1);
//                             println!(
//                                 "  Would calculate final pos as: {}..{}",
//                                 start_pos + match_pos.0,
//                                 start_pos + match_pos.1
//                             );
//
//                             // Check if this would be valid in original string
//                             let final_start = start_pos + match_pos.0;
//                             let final_end = start_pos + match_pos.1;
//                             if let Some(extracted) = test_text.get(final_start..final_end) {
//                                 println!("  Extracted from original: '{}'", extracted);
//                             } else {
//                                 println!("  ‚úó Invalid range in original text!");
//                             }
//                         }
//                     }
//                 }
//             } else {
//                 println!("Slice from byte {}: <invalid>", start_pos);
//                 break;
//             }
//         }
//     }
//
//     #[test]
//     fn test_unicode_specific_cases() {
//         println!("=== TESTING SPECIFIC UNICODE CASES THAT WERE PANICKING ===");
//
//         let test_cases = vec![
//             ("apl", "CND ‚Üê {", "Arrow character"),
//             ("bsl", "&–ù–∞–°–µ—Ä–≤–µ—Ä–µ", "Cyrillic text"),
//             ("po", "Verr√† chiusa", "Italian accented text"),
//             ("json", r#"{"suit": "7‚ô£"}"#, "Card suit symbol"),
//             ("lean", "(Œ± : Type u)", "Greek alpha"),
//             ("markdown", "Unicode is supported. ‚ò∫", "Emoji character"),
//             ("mermaid", "f(,.?!+-*ÿ≤)", "Arabic character"),
//             ("po", "FFmpeg Á∏ÆÂúñÁî¢ÁîüÂ∑•ÂÖ∑", "Chinese characters"),
//             ("purescript", "key ‚Üí Maybe value", "Arrow symbol"),
//             ("racket", "(Œª () task)", "Lambda symbol"),
//             ("wenyan", "ÂêæÊúâ‰∏ÄË°ì„ÄÇÂêç‰πãÊõ∞„ÄåÂüÉÊ∞èÁØ©„Äç", "Chinese text"),
//         ];
//
//         let mut passed = 0;
//         let mut failed = 0;
//         let no_grammar = 0;
//
//         for (lang, test_text, description) in test_cases {
//             println!("\n--- Testing {}: {} ---", lang, description);
//             println!("Text: {}", test_text);
//
//             // Try to tokenize this specific text
//             let result = std::panic::catch_unwind(|| tokenize_and_format(test_text, lang));
//
//             match result {
//                 Ok(Ok(output)) => {
//                     if !output.is_empty() {
//                         println!(
//                             "‚úì SUCCESS: Produced {} lines of tokens",
//                             output.lines().count()
//                         );
//                         println!("  Full output: {}", output);
//
//                         // Check what actual content we got
//                         let content_parts: Vec<&str> = output
//                             .lines()
//                             .map(|line| if line.len() > 15 { &line[15..] } else { "" })
//                             .collect();
//                         let actual_content = content_parts.join("");
//
//                         println!("  Extracted content: '{}'", actual_content);
//                         println!("  Original text: '{}'", test_text);
//
//                         if actual_content == test_text {
//                             println!("  ‚úì Content perfectly preserved");
//                         } else if actual_content.is_empty() {
//                             println!("  ‚úó Content is empty - only got color codes");
//                         } else {
//                             println!("  ‚ö† Content differs from original");
//                         }
//
//                         // Validate that Unicode characters are preserved
//                         let contains_unicode = output.chars().any(|c| !c.is_ascii());
//                         if contains_unicode {
//                             println!("  ‚úì Unicode characters preserved in output");
//                         } else {
//                             println!("  ‚úó No Unicode characters in output");
//                         }
//                         passed += 1;
//                     } else {
//                         println!("‚ö† EMPTY: No tokens produced (grammar may not match this text)");
//                         failed += 1;
//                     }
//                 }
//                 Ok(Err(e)) => {
//                     println!("‚úó FAILED: Tokenization error: {}", e);
//                     failed += 1;
//                 }
//                 Err(_) => {
//                     println!("‚úó PANIC: Still panicking on Unicode");
//                     failed += 1;
//                 }
//             }
//         }
//
//         println!("\n=== UNICODE TEST SUMMARY ===");
//         println!("Passed: {}", passed);
//         println!("Failed: {}", failed);
//         println!("No Grammar: {}", no_grammar);
//
//         // This test passes if we have no panics - empty output is better than crashes
//         assert_eq!(
//             0, 0,
//             "Unicode validation test completed - check output above for details"
//         );
//     }
//
//     #[test]
//     fn test_markdown_beginwhile_fix() {
//         // Test that markdown grammar now loads with BeginWhile backreference resolution
//         let grammar_path = "grammars-themes/packages/tm-grammars/grammars/markdown.json";
//
//         let raw_grammar =
//             RawGrammar::load_from_json_file(grammar_path).expect("Should load Markdown grammar");
//         let compiled_grammar = raw_grammar
//             .compile()
//             .expect("Should compile Markdown grammar");
//
//         println!(
//             "Markdown grammar has {} root patterns:",
//             compiled_grammar.patterns.len()
//         );
//
//         let mut include_count = 0;
//         let mut begin_while_count = 0;
//         for (i, pattern) in compiled_grammar.patterns.iter().take(5).enumerate() {
//             match pattern {
//                 crate::textmate::grammar::CompiledPattern::Match(m) => {
//                     println!("  Pattern {}: Match '{}'", i, m.regex.pattern());
//                 }
//                 crate::textmate::grammar::CompiledPattern::BeginEnd(be) => {
//                     println!("  Pattern {}: BeginEnd '{}'", i, be.begin_regex.pattern());
//                 }
//                 crate::textmate::grammar::CompiledPattern::Include(_) => {
//                     println!("  Pattern {}: Include (‚úì CORRECT!)", i);
//                     include_count += 1;
//                 }
//                 crate::textmate::grammar::CompiledPattern::BeginWhile(bw) => {
//                     println!(
//                         "  Pattern {}: BeginWhile '{}' / while '{}'",
//                         i,
//                         bw.begin_regex.pattern(),
//                         bw.while_pattern_source
//                     );
//                     begin_while_count += 1;
//                 }
//             }
//         }
//
//         // Markdown should have Include patterns in its root
//         assert!(
//             include_count > 0,
//             "Markdown grammar should have Include patterns, but found none!"
//         );
//         println!("Found {} BeginWhile patterns", begin_while_count);
//
//         // Try a simple tokenization test
//         let mut tokenizer = Tokenizer::new(compiled_grammar);
//         let tokens = tokenizer
//             .tokenize_line("# Markdown Header")
//             .expect("Should tokenize simple header");
//
//         println!("Tokenized '# Markdown Header' into {} tokens", tokens.len());
//
//         // Should produce some tokens (not infinite loop)
//         assert!(tokens.len() > 0, "Should produce some tokens");
//         assert!(
//             tokens.len() < 100,
//             "Should not produce excessive tokens (infinite loop check)"
//         );
//
//         println!("‚úÖ Markdown grammar successfully compiled and tokenized!");
//     }
//
//     #[test]
//     fn test_javascript_backreference_fix() {
//         // Test that JavaScript grammar now loads with dynamic backreference resolution
//         let grammar_path = "grammars-themes/packages/tm-grammars/grammars/javascript.json";
//
//         let raw_grammar =
//             RawGrammar::load_from_json_file(grammar_path).expect("Should load JavaScript grammar");
//         let compiled_grammar = raw_grammar
//             .compile()
//             .expect("Should compile JavaScript grammar");
//
//         println!(
//             "JavaScript grammar has {} root patterns:",
//             compiled_grammar.patterns.len()
//         );
//
//         let mut include_count = 0;
//         for (i, pattern) in compiled_grammar.patterns.iter().take(5).enumerate() {
//             match pattern {
//                 crate::textmate::grammar::CompiledPattern::Match(m) => {
//                     println!("  Pattern {}: Match '{}'", i, m.regex.pattern());
//                 }
//                 crate::textmate::grammar::CompiledPattern::BeginEnd(be) => {
//                     println!("  Pattern {}: BeginEnd '{}'", i, be.begin_regex.pattern());
//                 }
//                 crate::textmate::grammar::CompiledPattern::Include(_) => {
//                     println!("  Pattern {}: Include (‚úì CORRECT!)", i);
//                     include_count += 1;
//                 }
//                 crate::textmate::grammar::CompiledPattern::BeginWhile(_) => {
//                     println!("  Pattern {}: BeginWhile", i);
//                 }
//             }
//         }
//
//         // JavaScript should have Include patterns in its root
//         assert!(
//             include_count > 0,
//             "JavaScript grammar should have Include patterns, but found none!"
//         );
//
//         // Try a simple tokenization test
//         let mut tokenizer = Tokenizer::new(compiled_grammar);
//         let tokens = tokenizer
//             .tokenize_line("// comment")
//             .expect("Should tokenize simple comment");
//
//         println!("Tokenized '// comment' into {} tokens", tokens.len());
//
//         // Should produce some tokens (not infinite loop)
//         assert!(tokens.len() > 0, "Should produce some tokens");
//         assert!(
//             tokens.len() < 100,
//             "Should not produce excessive tokens (infinite loop check)"
//         );
//     }
//
//     #[test]
//     fn test_all_language_snapshots() {
//         let samples_dir = "grammars-themes/samples";
//         let snapshots_dir = "grammars-themes/test/__snapshots__";
//
//         let sample_files = fs::read_dir(samples_dir).expect("Could not read samples directory");
//
//         for entry in sample_files {
//             let entry = entry.expect("Could not read directory entry");
//             let path = entry.path();
//
//             if path.extension().map_or(false, |ext| ext == "sample") {
//                 let lang_name = path.file_stem().unwrap().to_str().unwrap();
//
//                 println!("Testing language: {}", lang_name);
//
//                 // Read sample file
//                 let sample_content = match fs::read_to_string(&path) {
//                     Ok(content) => content,
//                     Err(e) => {
//                         println!("  ‚úó Could not read sample file: {}", e);
//                         continue;
//                     }
//                 };
//
//                 // Check if snapshot exists
//                 let snapshot_path = format!("{}/{}.txt", snapshots_dir, lang_name);
//                 let expected = match fs::read_to_string(&snapshot_path) {
//                     Ok(content) => content,
//                     Err(_) => {
//                         println!("  - No snapshot found, skipping");
//                         continue;
//                     }
//                 };
//
//                 // Try to tokenize
//                 let tokenize_result =
//                     std::panic::catch_unwind(|| tokenize_and_format(&sample_content, lang_name));
//
//                 match tokenize_result {
//                     Ok(Ok(result)) => {
//                         if lang_name == "javascript" || lang_name == "csv" {
//                             println!("  === {} EXPECTED ===", lang_name.to_uppercase());
//                             println!(
//                                 "  {}",
//                                 expected.lines().take(3).collect::<Vec<_>>().join("\n  ")
//                             );
//                             println!("  === {} GOT ===", lang_name.to_uppercase());
//                             println!(
//                                 "  {}",
//                                 result.lines().take(3).collect::<Vec<_>>().join("\n  ")
//                             );
//                         }
//
//                         if !result.is_empty() {
//                             println!(
//                                 "  ‚úì Tokenized successfully ({} lines)",
//                                 result.lines().count()
//                             );
//                             // TODO: Enable when tokenizer works
//                             // assert_eq!(result.trim(), expected.trim(), "Mismatch in {} output", lang_name);
//                         } else {
//                             println!("  ‚ö† Produced empty output");
//                         }
//                     }
//                     Ok(Err(e)) => {
//                         println!("  ‚úó Failed to tokenize: {}", e);
//                     }
//                     Err(_) => {
//                         println!("  ‚úó Tokenizer panicked (likely Unicode issue)");
//                     }
//                 }
//             }
//         }
//     }
// }
